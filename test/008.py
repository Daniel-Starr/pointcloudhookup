#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹è¿è¡Œçš„è½»é‡çº§æ†å¡”æå–å·¥å…·
ä½œè€…: AI Assistant
æ—¥æœŸ: 2025å¹´
ç”¨é€”: ä»ç‚¹äº‘æ•°æ®ä¸­æå–æ†å¡”ä¿¡æ¯ï¼ŒèŠ‚çº¦ç³»ç»Ÿèµ„æº

ä½¿ç”¨æ–¹æ³•:
1. ç›´æ¥è¿è¡Œ: python standalone_tower_extraction.py
2. å‘½ä»¤è¡Œ: python standalone_tower_extraction.py input.las
3. å¯¼å…¥ä½¿ç”¨: from standalone_tower_extraction import extract_towers
"""

import sys
import os
import argparse
from pathlib import Path
import time
import gc
import warnings

warnings.filterwarnings('ignore')


# æ£€æŸ¥å’Œå¯¼å…¥ä¾èµ–
def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–åº“"""
    missing_deps = []

    try:
        import numpy as np
    except ImportError:
        missing_deps.append('numpy')

    try:
        import pandas as pd
    except ImportError:
        missing_deps.append('pandas')

    try:
        import laspy
    except ImportError:
        missing_deps.append('laspy')

    try:
        from sklearn.cluster import DBSCAN
    except ImportError:
        missing_deps.append('scikit-learn')

    try:
        import trimesh
    except ImportError:
        missing_deps.append('trimesh')

    try:
        import psutil
    except ImportError:
        missing_deps.append('psutil')

    if missing_deps:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åº“:")
        for dep in missing_deps:
            print(f"   - {dep}")
        print("\nğŸ“¦ è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(missing_deps)}")
        return False

    print("âœ… æ‰€æœ‰ä¾èµ–åº“æ£€æŸ¥é€šè¿‡")
    return True


# åªæœ‰é€šè¿‡æ£€æŸ¥æ‰å¯¼å…¥
if not check_dependencies():
    sys.exit(1)

import numpy as np
import pandas as pd
import laspy
from sklearn.cluster import DBSCAN
import trimesh
import psutil
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Tuple, Optional, Callable

# å°è¯•å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import hdbscan

    HAS_HDBSCAN = True
    print("âœ… HDBSCANé«˜æ€§èƒ½èšç±»å·²å¯ç”¨")
except ImportError:
    HAS_HDBSCAN = False
    print("â„¹ï¸ ä½¿ç”¨æ ‡å‡†DBSCANèšç±»")

try:
    import open3d as o3d

    HAS_OPEN3D = True
    print("âœ… Open3Då¯è§†åŒ–æ”¯æŒå·²å¯ç”¨")
except ImportError:
    HAS_OPEN3D = False
    print("â„¹ï¸ æ— Open3Dæ”¯æŒï¼Œè·³è¿‡å¯è§†åŒ–åŠŸèƒ½")


class StandaloneTowerExtractor:
    """ç‹¬ç«‹çš„æ†å¡”æå–å™¨"""

    def __init__(self, max_memory_percent: int = 30, max_threads: int = 6):
        self.max_memory_percent = max_memory_percent
        self.max_threads = max_threads
        self.total_memory_gb = psutil.virtual_memory().total / (1024 ** 3)
        self.memory_limit_gb = self.total_memory_gb * max_memory_percent / 100

        print(f"ğŸ¯ ç‹¬ç«‹æ†å¡”æå–å™¨åˆå§‹åŒ–:")
        print(f"   ç³»ç»Ÿå†…å­˜: {self.total_memory_gb:.1f}GB")
        print(f"   ä½¿ç”¨é™åˆ¶: {self.memory_limit_gb:.1f}GB ({max_memory_percent}%)")
        print(f"   çº¿ç¨‹é™åˆ¶: {max_threads}")

    def log_progress(self, message: str, step: int = None, total: int = None):
        """æ—¥å¿—è¾“å‡º"""
        if step and total:
            percentage = int(step * 100 / total)
            print(f"[{percentage:3d}%] {message}")
        else:
            print(f"â„¹ï¸ {message}")

    def monitor_memory(self, operation: str = ""):
        """ç›‘æ§å†…å­˜ä½¿ç”¨"""
        memory_info = psutil.virtual_memory()
        used_gb = (memory_info.total - memory_info.available) / (1024 ** 3)
        percent = memory_info.percent

        print(f"ğŸ’¾ {operation} å†…å­˜ä½¿ç”¨: {used_gb:.1f}GB ({percent:.1f}%)")

        if percent > 80:
            print("âš ï¸ å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å…³é—­å…¶ä»–ç¨‹åº")

        return percent

    def read_las_file(self, file_path: str) -> Tuple[np.ndarray, Dict]:
        """è¯»å–LASæ–‡ä»¶"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        self.log_progress(f"è¯»å–æ–‡ä»¶: {file_path}")
        self.log_progress(f"æ–‡ä»¶å¤§å°: {file_size_mb:.1f}MB")

        try:
            with laspy.open(file_path) as las_file:
                header_info = {
                    "scales": las_file.header.scales,
                    "offsets": las_file.header.offsets,
                    "point_format": las_file.header.point_format,
                    "version": las_file.header.version,
                    "point_count": las_file.header.point_count
                }

                # æ ¹æ®æ–‡ä»¶å¤§å°å†³å®šè¯»å–ç­–ç•¥
                if file_size_mb > 500:  # å¤§äº500MBçš„æ–‡ä»¶åˆ†å—è¯»å–
                    points = self._read_large_file(las_file)
                else:
                    las_data = las_file.read()
                    points = np.stack([las_data.x, las_data.y, las_data.z], axis=1).astype(np.float32)
                    del las_data

                # è®¡ç®—å¹¶åº”ç”¨ä¸­å¿ƒåŒ–
                centroid = np.mean(points, axis=0)
                points_centered = points - centroid
                header_info["centroid"] = centroid

                self.log_progress(f"è¯»å–å®Œæˆ: {len(points_centered):,} ä¸ªç‚¹")
                self.monitor_memory("æ–‡ä»¶è¯»å–å")

                return points_centered, header_info

        except Exception as e:
            raise Exception(f"è¯»å–LASæ–‡ä»¶å¤±è´¥: {str(e)}")

    def _read_large_file(self, las_file) -> np.ndarray:
        """åˆ†å—è¯»å–å¤§æ–‡ä»¶"""
        self.log_progress("æ£€æµ‹åˆ°å¤§æ–‡ä»¶ï¼Œä½¿ç”¨åˆ†å—è¯»å–...")

        chunk_size = 500_000  # æ¯æ¬¡è¯»å–50ä¸‡ç‚¹
        points_list = []
        total_points = 0

        for i, chunk in enumerate(las_file.chunk_iterator(chunk_size)):
            chunk_points = np.stack([chunk.x, chunk.y, chunk.z], axis=1).astype(np.float32)
            points_list.append(chunk_points)
            total_points += len(chunk_points)

            self.log_progress(f"è¯»å–å— {i + 1}: {total_points:,} ç‚¹", i + 1, 10)

            # å†…å­˜æ£€æŸ¥
            if self.monitor_memory(f"å—{i + 1}è¯»å–å") > 75:
                self.log_progress("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œåœæ­¢è¯»å–æ›´å¤šæ•°æ®")
                break

            # é™åˆ¶æœ€å¤§å—æ•°
            if i >= 20:  # æœ€å¤š20å—
                self.log_progress("è¾¾åˆ°æœ€å¤§å¤„ç†å—æ•°ï¼Œåœæ­¢è¯»å–")
                break

        combined_points = np.vstack(points_list)
        del points_list
        gc.collect()

        return combined_points

    def height_filter(self, points: np.ndarray) -> np.ndarray:
        """é«˜åº¦è¿‡æ»¤"""
        self.log_progress("æ‰§è¡Œé«˜åº¦è¿‡æ»¤...")

        z_values = points[:, 2]
        base_height = np.percentile(z_values, 20)  # ä½¿ç”¨20ç™¾åˆ†ä½ä½œä¸ºåœ°é¢é«˜åº¦
        height_threshold = 2.5  # é«˜åº¦é˜ˆå€¼

        mask = z_values > (base_height + height_threshold)
        filtered_points = points[mask]

        self.log_progress(f"é«˜åº¦è¿‡æ»¤å®Œæˆ: {len(points):,} -> {len(filtered_points):,} ç‚¹")
        return filtered_points

    def adaptive_downsample(self, points: np.ndarray) -> np.ndarray:
        """è‡ªé€‚åº”é™é‡‡æ ·"""
        if len(points) <= 1_000_000:  # å°äº100ä¸‡ç‚¹ä¸é™é‡‡æ ·
            return points

        # è®¡ç®—ç›®æ ‡ç‚¹æ•°ï¼ˆåŸºäºå†…å­˜é™åˆ¶ï¼‰
        max_points = int(self.memory_limit_gb * 1024 ** 3 / 24)  # æ¯ç‚¹çº¦24å­—èŠ‚
        max_points = min(max_points, 2_000_000)  # æœ€å¤š200ä¸‡ç‚¹

        if len(points) > max_points:
            ratio = max_points / len(points)
            indices = np.random.choice(len(points), max_points, replace=False)
            sampled_points = points[indices]

            self.log_progress(f"è‡ªé€‚åº”é™é‡‡æ ·: {len(points):,} -> {len(sampled_points):,} ç‚¹ (æ¯”ä¾‹: {ratio:.1%})")
            return sampled_points

        return points

    def cluster_points(self, points: np.ndarray, eps: float = 10.0, min_samples: int = 50) -> np.ndarray:
        """ç‚¹äº‘èšç±»"""
        self.log_progress(f"å¼€å§‹èšç±»åˆ†æ (eps={eps}, min_samples={min_samples})...")
        self.monitor_memory("èšç±»å‰")

        # é€‰æ‹©èšç±»ç®—æ³•
        if HAS_HDBSCAN and len(points) < 800_000:
            # å°æ•°æ®é›†ä½¿ç”¨HDBSCAN
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_samples,
                algorithm='boruvka_kdtree',
                n_jobs=self.max_threads
            )
            self.log_progress("ä½¿ç”¨HDBSCANèšç±»ç®—æ³•")
        else:
            # å¤§æ•°æ®é›†ä½¿ç”¨DBSCAN
            clusterer = DBSCAN(
                eps=eps,
                min_samples=min_samples,
                algorithm='kd_tree',
                n_jobs=self.max_threads
            )
            self.log_progress("ä½¿ç”¨DBSCANèšç±»ç®—æ³•")

        # æ‰§è¡Œèšç±»
        labels = clusterer.fit_predict(points)

        # ç»Ÿè®¡ç»“æœ
        unique_labels = set(labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = list(labels).count(-1)

        self.log_progress(f"èšç±»å®Œæˆ: {n_clusters} ä¸ªç°‡, {n_noise} ä¸ªå™ªå£°ç‚¹")
        self.monitor_memory("èšç±»å")

        return labels

    def detect_towers(self, points: np.ndarray, labels: np.ndarray,
                      min_height: float = 12.0, max_width: float = 60.0,
                      min_width: float = 6.0, aspect_ratio_threshold: float = 0.6) -> List[Dict]:
        """æ£€æµ‹æ†å¡”"""
        self.log_progress("å¼€å§‹æ†å¡”æ£€æµ‹...")

        unique_labels = set(labels) - {-1}
        towers = []

        for i, label in enumerate(unique_labels):
            if i % 100 == 0:  # æ¯100ä¸ªç°‡æ˜¾ç¤ºè¿›åº¦
                self.log_progress(f"æ£€æµ‹è¿›åº¦: {i}/{len(unique_labels)}", i, len(unique_labels))

            try:
                mask = labels == label
                cluster_points = points[mask]

                if len(cluster_points) < 30:  # ç‚¹æ•°å¤ªå°‘
                    continue

                # è®¡ç®—è¾¹ç•Œæ¡†
                min_coords = np.min(cluster_points, axis=0)
                max_coords = np.max(cluster_points, axis=0)
                extents = max_coords - min_coords

                height = extents[2]
                width = max(extents[0], extents[1])

                if width <= 0:
                    continue

                aspect_ratio = height / width

                # æ†å¡”åˆ¤æ–­æ¡ä»¶
                if (height > min_height and
                        min_width < width < max_width and
                        aspect_ratio > aspect_ratio_threshold):
                    center = (min_coords + max_coords) / 2

                    tower = {
                        "id": len(towers) + 1,
                        "center": center,
                        "height": height,
                        "width": width,
                        "aspect_ratio": aspect_ratio,
                        "point_count": len(cluster_points),
                        "extents": extents
                    }

                    towers.append(tower)

            except Exception:
                continue

        self.log_progress(f"æ†å¡”æ£€æµ‹å®Œæˆ: å‘ç° {len(towers)} ä¸ªå€™é€‰æ†å¡”")
        return towers

    def remove_duplicates(self, towers: List[Dict], distance_threshold: float = 25.0) -> List[Dict]:
        """å»é™¤é‡å¤æ£€æµ‹"""
        if len(towers) <= 1:
            return towers

        self.log_progress("å»é™¤é‡å¤æ£€æµ‹...")

        # ç®€å•çš„è·ç¦»å»é‡
        unique_towers = []

        for tower in towers:
            is_duplicate = False

            for existing in unique_towers:
                distance = np.linalg.norm(tower['center'] - existing['center'])
                if distance < distance_threshold:
                    # ä¿ç•™ç‚¹æ•°æ›´å¤šçš„å¡”
                    if tower['point_count'] <= existing['point_count']:
                        is_duplicate = True
                        break
                    else:
                        unique_towers.remove(existing)
                        break

            if not is_duplicate:
                unique_towers.append(tower)

        removed_count = len(towers) - len(unique_towers)
        if removed_count > 0:
            self.log_progress(f"å»é‡å®Œæˆ: ç§»é™¤ {removed_count} ä¸ªé‡å¤æ£€æµ‹")

        return unique_towers

    def save_results(self, towers: List[Dict], header_info: Dict, output_dir: str = "output") -> str:
        """ä¿å­˜ç»“æœ"""
        if not towers:
            self.log_progress("æ— æ†å¡”æ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
            return None

        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(output_dir).mkdir(exist_ok=True)

        # æ¢å¤åŸå§‹åæ ‡ç³»
        centroid = header_info["centroid"]
        for tower in towers:
            tower["center"] += centroid

        # å‡†å¤‡Excelæ•°æ®
        excel_data = []
        for tower in towers:
            excel_data.append({
                "æ†å¡”ID": f"T{tower['id']:03d}",
                "Xåæ ‡": tower["center"][0],
                "Yåæ ‡": tower["center"][1],
                "Zåæ ‡": tower["center"][2],
                "æ†å¡”é«˜åº¦": tower["height"],
                "æ†å¡”å®½åº¦": tower["width"],
                "é•¿å®½æ¯”": tower["aspect_ratio"],
                "ç‚¹æ•°": tower["point_count"]
            })

        # ä¿å­˜Excelæ–‡ä»¶
        df = pd.DataFrame(excel_data)
        excel_path = os.path.join(output_dir, "æ†å¡”æ£€æµ‹ç»“æœ.xlsx")
        df.to_excel(excel_path, index=False, engine='openpyxl')

        self.log_progress(f"ç»“æœå·²ä¿å­˜: {excel_path}")
        return excel_path


def extract_towers_standalone(input_file: str,
                              output_dir: str = "output",
                              max_memory_percent: int = 30,
                              max_threads: int = 6,
                              eps: float = 10.0,
                              min_samples: int = 50,
                              min_height: float = 12.0,
                              max_width: float = 60.0,
                              min_width: float = 6.0,
                              aspect_ratio_threshold: float = 0.6) -> List[Dict]:
    """
    ç‹¬ç«‹è¿è¡Œçš„æ†å¡”æå–å‡½æ•°

    å‚æ•°:
        input_file: è¾“å…¥LASæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        max_memory_percent: æœ€å¤§å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯” (é»˜è®¤30%)
        max_threads: æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤6)
        eps: DBSCANèšç±»åŠå¾„
        min_samples: æœ€å°æ ·æœ¬æ•°
        min_height: æœ€å°æ†å¡”é«˜åº¦
        max_width: æœ€å¤§æ†å¡”å®½åº¦
        min_width: æœ€å°æ†å¡”å®½åº¦
        aspect_ratio_threshold: é•¿å®½æ¯”é˜ˆå€¼

    è¿”å›:
        æ£€æµ‹åˆ°çš„æ†å¡”åˆ—è¡¨
    """

    print("=" * 60)
    print("ğŸ—ï¸ ç‹¬ç«‹æ†å¡”æå–å·¥å…·")
    print("=" * 60)

    start_time = time.time()

    # åˆå§‹åŒ–æå–å™¨
    extractor = StandaloneTowerExtractor(max_memory_percent, max_threads)

    try:
        # 1. è¯»å–æ–‡ä»¶
        points, header_info = extractor.read_las_file(input_file)

        # 2. é«˜åº¦è¿‡æ»¤
        filtered_points = extractor.height_filter(points)
        del points  # é‡Šæ”¾åŸå§‹ç‚¹äº‘å†…å­˜
        gc.collect()

        # 3. è‡ªé€‚åº”é™é‡‡æ ·
        sampled_points = extractor.adaptive_downsample(filtered_points)
        if len(sampled_points) < len(filtered_points):
            del filtered_points
            filtered_points = sampled_points
        gc.collect()

        # 4. èšç±»åˆ†æ
        labels = extractor.cluster_points(filtered_points, eps, min_samples)

        # 5. æ†å¡”æ£€æµ‹
        towers = extractor.detect_towers(
            filtered_points, labels,
            min_height, max_width, min_width, aspect_ratio_threshold
        )

        # 6. å»é‡
        unique_towers = extractor.remove_duplicates(towers)

        # 7. ä¿å­˜ç»“æœ
        output_file = extractor.save_results(unique_towers, header_info, output_dir)

        # æ€»ç»“
        elapsed_time = time.time() - start_time
        final_memory = extractor.monitor_memory("å¤„ç†å®Œæˆå")

        print("\n" + "=" * 60)
        print("âœ… å¤„ç†å®Œæˆ!")
        print(f"â±ï¸  æ€»ç”¨æ—¶: {elapsed_time:.1f}ç§’")
        print(f"ğŸ—ï¸  æ£€æµ‹åˆ°æ†å¡”: {len(unique_towers)} ä¸ª")
        print(f"ğŸ’¾ æœ€ç»ˆå†…å­˜ä½¿ç”¨: {final_memory:.1f}%")
        if output_file:
            print(f"ğŸ“„ ç»“æœæ–‡ä»¶: {output_file}")
        print("=" * 60)

        return unique_towers

    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return []
    finally:
        # æœ€ç»ˆå†…å­˜æ¸…ç†
        gc.collect()


def main():
    """å‘½ä»¤è¡Œä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç‹¬ç«‹è¿è¡Œçš„æ†å¡”æå–å·¥å…·")
    parser.add_argument("input", nargs="?", help="è¾“å…¥LASæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", default="output", help="è¾“å‡ºç›®å½• (é»˜è®¤: output)")
    parser.add_argument("-m", "--memory", type=int, default=30, help="æœ€å¤§å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯” (é»˜è®¤: 30)")
    parser.add_argument("-t", "--threads", type=int, default=6, help="æœ€å¤§çº¿ç¨‹æ•° (é»˜è®¤: 6)")
    parser.add_argument("--eps", type=float, default=10.0, help="èšç±»åŠå¾„ (é»˜è®¤: 10.0)")
    parser.add_argument("--min-samples", type=int, default=50, help="æœ€å°æ ·æœ¬æ•° (é»˜è®¤: 50)")
    parser.add_argument("--min-height", type=float, default=12.0, help="æœ€å°æ†å¡”é«˜åº¦ (é»˜è®¤: 12.0)")
    parser.add_argument("--max-width", type=float, default=60.0, help="æœ€å¤§æ†å¡”å®½åº¦ (é»˜è®¤: 60.0)")

    args = parser.parse_args()

    if not args.input:
        # äº¤äº’å¼æ¨¡å¼
        print("ğŸ” äº¤äº’å¼æ¨¡å¼")
        print("è¯·è¾“å…¥LASæ–‡ä»¶è·¯å¾„:")
        input_file = input().strip().strip('"')  # å»é™¤å¯èƒ½çš„å¼•å·
    else:
        input_file = args.input

    if not os.path.exists(input_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return

    # è¿è¡Œæå–
    towers = extract_towers_standalone(
        input_file=input_file,
        output_dir=args.output,
        max_memory_percent=args.memory,
        max_threads=args.threads,
        eps=args.eps,
        min_samples=args.min_samples,
        min_height=args.min_height,
        max_width=args.max_width
    )

    if towers:
        print(f"\nğŸ‰ æˆåŠŸæ£€æµ‹åˆ° {len(towers)} ä¸ªæ†å¡”!")
        print("è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹è¾“å‡ºçš„Excelæ–‡ä»¶ã€‚")
    else:
        print("\nğŸ˜” æœªæ£€æµ‹åˆ°æ†å¡”ï¼Œè¯·å°è¯•è°ƒæ•´å‚æ•°ã€‚")


if __name__ == "__main__":
    main()